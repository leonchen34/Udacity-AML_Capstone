{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice, uniform\n",
    "from azureml.core import Environment, ScriptRunConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "\"Airbnb for Boston with fraud detcetion\" data was downloaded from Kaggle with the following link:\n",
    "\n",
    "https://www.kaggle.com/datasets/hawkingcr/airbnb-for-boston-with-fraud-detection/download?datasetVersionNumber=1\n",
    "\n",
    "The downloaded file was saved as \"output.csv\" in the \"data\" directory. The dataset aims to classify whether an Airbnb listing is a fraud or not.\n",
    "\n",
    "A notebook file named \"data_process.ipyng\" was created to perform some pre-processing on the data. Firstly, a correlation analysis was conducted with the target column \"fraud\" to identify and remove some non-significant features. Next, the data was split into \"train.csv\" and \"test.csv\" sets, and the balance of the training data was examined. Due to the class imbalance in the training target, an upsampling technique was applied to address this imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from scripts.data_set import getTrainingDataset, getTestDataset\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment_name = 'udacity-aml-capstone-hyperparameter'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "train_ds = getTrainingDataset(ws)\n",
    "print(train_ds.to_pandas_dataframe().head())\n",
    "test_ds = getTestDataset(ws)\n",
    "print(test_ds.to_pandas_dataframe().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach an AmlCompute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"my-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_D2_V2\", max_nodes=4\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "Logistic Regression model is employed here. There are two tuning parameters, inverse of regularization strength and max iteration.  I picked Random sampling method over Grid sampling for both continuous and discrete, and over Bayesian sampling for efficiency.\n",
    "\n",
    "For early termination policy, agressive BanditPolicy is chosen with a small slack_factor of 0.1.\n",
    "\n",
    "Since the data is imbalanced, primary metric \"AUC_wighted\" is chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Early termination policy. \n",
    "early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "# Tuning parameters\n",
    "param_sampling = RandomParameterSampling(\n",
    "    {\n",
    "        '--C': uniform(0.5,1.5),\n",
    "        '--max_iter': choice(75, 100, 125)\n",
    "    }\n",
    ")\n",
    "\n",
    "if \"scripts\" not in os.listdir():\n",
    "    os.mkdir(\"./scripts\")\n",
    "\n",
    "# Setup environment for your training run\n",
    "sklearn_env = Environment.from_conda_specification(name='sklearn-env', file_path='conda_dependencies.yml')\n",
    "\n",
    "estimator = ScriptRunConfig(source_directory='./scripts',\n",
    "                      script='logistic_regression.py',\n",
    "                      compute_target=compute_target,\n",
    "                      environment=sklearn_env)\n",
    "\n",
    "# Create a HyperDriveConfig using the src object, hyperparameter sampler, and policy.\n",
    "hyperdrive_run_config = HyperDriveConfig(run_config=estimator,\n",
    "                                     hyperparameter_sampling=param_sampling,\n",
    "                                     policy=early_termination_policy,\n",
    "                                     # primary_metric_name='Accuracy',\n",
    "                                     primary_metric_name='AUC_weighted',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=20,\n",
    "                                     max_concurrent_runs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "hyperdrive_run = experiment.submit(config=hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get best run\n",
    "\n",
    "hyperdrive_run.wait_for_completion(show_output=True)\n",
    "assert(hyperdrive_run.get_status() == \"Completed\")\n",
    "\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best run\n",
    "\n",
    "# print(best_run.get_metrics())\n",
    "print(best_run.get_environment())\n",
    "print(best_run.get_file_names())\n",
    "# print(best_run.get_details())\n",
    "print(best_run.get_properties())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "output_dir = \"./outputs\"\n",
    "if os.path.exists(output_dir) == False:\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "best_metrics = best_run.get_metrics()\n",
    "#best_metrics_file = output_dir + \"/hyperdrive_metircs.json\"\n",
    "#joblib.dump(best_metrics,best_metrics_file)\n",
    "    \n",
    "# download best model\n",
    "\n",
    "best_model_name = output_dir + \"/best_model.pkl\"\n",
    "best_run.download_file(\"outputs/model.pkl\",best_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = joblib.load(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_metrics)\n",
    "print(best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "src_dir = \"./data\"\n",
    "test_file = src_dir + \"/test.csv\";\n",
    "df_test = pd.read_csv(test_file)\n",
    "df_test = df_test[pd.notnull(df_test['fraud'])]\n",
    "\n",
    "y_test = df_test['fraud']\n",
    "X_test = df_test.drop(['fraud'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, ypred)\n",
    "\n",
    "pd.DataFrame(cm).style.background_gradient(cmap='Blues', low=0, high=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Registration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"airbnb-boston-hyperparameter\"\n",
    "\n",
    "#description = \"logistic_regression model on Airbnb boston to predict fraud listing\"\n",
    "#registered_model = best_run.register_model(model_name=model_name)\n",
    "registered_model = best_run.register_model(model_name=model_name,model_path='outputs/model')\n",
    "\n",
    "print(\"registered model: \",registered_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webservice Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "script_file_name = \"./scripts/score.py\"\n",
    "inference_config = InferenceConfig(\n",
    "    environment=best_run.get_environment(), entry_script=script_file_name\n",
    ")\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=2,\n",
    "    memory_gb=2,\n",
    "    tags={\"type\": \"logistic regression\"},\n",
    "    description=\"Logistic Regression Classification\",\n",
    ")\n",
    "\n",
    "aci_service_name = model_name.lower()\n",
    "print(aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [registered_model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.scoring_uri)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a request to the web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import requests\n",
    "import json\n",
    "\n",
    "X_test_json = X_test.to_json(orient=\"records\")\n",
    "#data = '{\"data\": ' + X_test_json + \"}\"\n",
    "data = '{\"data\": ' + X_test_json + ', \"method\": \"predict\"}'\n",
    "#print(\"test data:\", data)\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "resp = requests.post(aci_service.scoring_uri, data, headers=headers)\n",
    "\n",
    "y_pred = json.loads(json.loads(resp.text))[\"result\"]\n",
    "#print(y_pred)\n",
    "\n",
    "#print(y_test)\n",
    "actual = array(y_test)\n",
    "#actual = actual[:, 0]\n",
    "print(len(y_pred), \" \", len(actual))\n",
    "#print(actual)\n",
    "\n",
    "cm = confusion_matrix(actual, ypred)\n",
    "\n",
    "pd.DataFrame(cm).style.background_gradient(cmap='Blues', low=0, high=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the logs of the web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.delete()\n",
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
